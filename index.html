<!DOCTYPE html>
<html>

<head>
  <title>Deep Learning for Intrusion Detection</title>
  <meta charset="UTF-8" />
  <meta name="description" content="This is the website for Lucas Carr and Christopher Lamprecht's honours projects" />
  <meta http-equiv="content-type" content="text/html;charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="author" content="Lucas Carr & Christopher Lamprecht" />
  <link rel="stylesheet" type="text/css" href="styles.css" />
  <script src="script.js"></script>
</head>

<body>
  <h1 class="Title">Deep Learning for Intrusion Detection</h1>

  <div class="header">
    <!-- <div class="project-description"> An evaluation of deep learning approaches to intrusion detection on encrypted network traffic -->
    <!-- </div> -->

    <div class="intro"> DL4ID is a UCT Honours Project, created by Lucas Carr and Christopher Lamprecht </div>
    <div class="intro"> Supervised by Dr Josiah Chavula </div>

  </div>

  <div class="container-top">
    <div class="tab-container">
      <div class="tab active" data-tab="tab1">Overview</div>
      <div class="tab" data-tab="tab2">Botnet Detection</div>
      <div class="tab" data-tab="tab3">Ransomware</div>
      <div class="tab" data-tab="tab4">Deliverables</div>
      <div class="tab" data-tab="tab5">Contact</div>
    </div>

    <div class="centered-hr"></div>
  </div>

  <div class="container-mid">
    <div id="tab1-content" class="tab-content" style="display: block;">
      <p> Insert an overview of the problem </p>

    </div>

    <!-- Content for Tab 2 -->
    <div id="tab2-content" class="tab-content">

      <p class="Abstract"> Abstract </p>

      <p>Detection of malicious traffic on a network is critical to ensuring the
        safety and security of internet systems. Classical approaches to this
        task increasingly struggle with modern networking procedures, like
        encryption. Deep learning (DL) offers an alternative approach to traffic
        classification problems.We address two major problem classes: (1) botnet
        detection and (2) botnet family classification. For each problem, we
        explore five implementations of DL architectures: a multi-layer perceptron
        (MLP), shallow and deep convolutional neural network (CNN v1
        and CNN v2), an autoencoder (AE) and an autoencoder + convolutional
        neural network (AE+CNN). We recognise a lack of surrounding literature
        which consider the computational requirements as an important
        aspect of model evaluation. Consequently, our evaluation of models
        for each respective problem class is based on the classification and
        computational performance of each model. We further investigate the
        effect of training the models on an input with a reduced feature space,
        where we discuss the impact this has in terms of a trade-off between
        computational and classification performance. For botnet detection,
        we find that all models attain good (≥ 0.979 accuracy) classification
        performance on a normal testing set; however, this performance drops
        fairly significantly when evaluated on a set of unknown botnet families.
        Furthermore, we observed a clear trend between increased feature
        space and memory utilisation, while finding no evidence of a trend
        inference time and feature space. For botnet classification, we found
        that models which implement CNN architectures outperform others
        by a significant margin (≈ 6 percentage points). We observe the same
        trend between feature space and memory utilisation, and absence of
        apparent relationship between feature space and inference time.</p>
    </div>

    <!-- Content for Tab 3 -->
    <div id="tab3-content" class="tab-content">
      <p class="Abstract"> Abstract </p>
      <p>A recent increase in ransomware attacks has necessitated tools that can detect threats within a computer
        network.
        Machine learning (ML) has been proposed as a solution to detect ransomware within network traffic. However,
        whilst many ML models are developed to obtain high classification accuracy, we also test their runtime memory
        performance and inference times. We further test the effect training the models on a reduced feature space has
        on the classification and computational metrics. In this study, we evaluate multi-layer perceptrons (MLP),
        1-dimensional and 2-dimensional convolutional neural networks (CNN) and long short-term memory (LSTM).
        Experiments are performed to give insight into each model's ability to reduce its false positives (FPs). We find
        a consistent increase in memory usage as the feature space increases and that the lowest amount of FPs is
        obtained when using 50% to 66% of the features. The study evaluates 17 models for their effectiveness in
        classifying ransomware network traffic. Efficiency in this context is considered a trade-off between accuracy,
        false positives, runtime memory usage, and inference time.</p>
      <p class="Abstract"> Research Objectives </p>
      <ol>
        <li>Assess the performance of MLP, 1D CNN, 2D CNN, and LSTM models in classifying encrypted network traffic as
          benign or ransomware, considering the accuracy, false positive/negative rate, inference time, and runtime
          memory usage of the models.</li>
        <li>Investigate the impact of using a window of samples on the accuracy, false positive rate, inference time,
          and runtime memory consumption on an LSTM by grouping multiple samples into one sample.</li>
        <li>Investigate the impact of dimensionality reduction on accuracy, false positive rate, inference time, and
          runtime memory consumption across the deep learning architectures (MLP, 1D CNN, 2D CNN, and LSTM) by
          decreasing the feature space from 30 samples down to 20, 15, and 5 samples.</li>
      </ol>
      <p class="Abstract"> Dataset </p>
      For this project, the <a href="http://dataset.tlm.unavarra.es/ransomware/">Open Access Dataset</a> (OA) was used.

      The OA dataset includes over 120 hours of ransomware traffic, comprising 94 samples from 43 different ransomware
      families, some of which have multiple samples. The dataset spans from 2015 to 2021, with varying packet counts and
      traffic durations among samples.
      <br>
      <br>
      The benign dataset in OA consists of normal network traffic generated by approximately 300 users on a campus
      network over one week, excluding weekends. Each day's traffic is segregated into its own subset.
      <br>
      <br>
      For modeling purposes, the data is divided into four groups: the train set, validation set, test set, and extra
      test set. These subsets include both benign and ransomware network traffic, except for the extra test set, which
      exclusively contains benign traffic for false positive testing. The test set contains previously unseen network
      traffic but also includes ransomware from the same families present in the train set.
      <br>
      <p class="Abstract">Experiments</p>
      <p>The table below presents the experiments performed to address the research objectives of the project.</p>
      <table class="tg">
        <thead>
          <tr>
            <th class="tg-0pky"><span style="font-weight:bold">Experiment</span></th>
            <th class="tg-0pky"><span style="font-weight:bold">Description</span></th>
            <th class="tg-0pky"><span style="font-weight:bold">Brief Deception Of Target Objective</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-0pky">1</td>
            <td class="tg-0pky">Establishing a simple MLP baseline and <br>training DL models</td>
            <td class="tg-0pky">Objective 1: Evaluate ML models for&nbsp;&nbsp;encrypted<br>traffic classification</td>
          </tr>
          <tr>
            <td class="tg-0pky">2</td>
            <td class="tg-0pky">Varying window on input.</td>
            <td class="tg-0pky">Objective 2: Sliding Window</td>
          </tr>
          <tr>
            <td class="tg-0pky">3</td>
            <td class="tg-0pky">Testing for false positives</td>
            <td class="tg-0pky">Objective 1&amp;2</td>
          </tr>
          <tr>
            <td class="tg-0lax">4</td>
            <td class="tg-0lax">Varying input feature-length</td>
            <td class="tg-0lax">Objective 3: Dimensionality reduction effect on models</td>
          </tr>
        </tbody>
      </table>
      <br>
      <p class="Abstract"> Results </p>
      <table class="tg">
        <thead>
          <tr>
            <th class="tg-0pky"></th>
            <th class="tg-0pky"><span style="font-weight:bold">%Accuracy</span></th>
            <th class="tg-fymr"><span style="font-weight:bold">False Positives</span></th>
            <th class="tg-0pky"><span style="font-weight:bold">False Negatives</span></th>
            <th class="tg-0pky"><span style="font-weight:bold">Inference Time (ms)</span></th>
            <th class="tg-0pky"><span style="font-weight:bold">Memory Usage (MiB)</span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-0pky"><span style="font-weight:bold">MLP</span></td>
            <td class="tg-0pky">99.20</td>
            <td class="tg-0pky">629</td>
            <td class="tg-0pky">229</td>
            <td class="tg-0pky">23</td>
            <td class="tg-0pky">1961</td>
          </tr>
          <tr>
            <td class="tg-0pky"><span style="font-weight:bold">1D CNN</span></td>
            <td class="tg-0pky">99.18</td>
            <td class="tg-0pky">2</td>
            <td class="tg-0pky">48</td>
            <td class="tg-0pky">34</td>
            <td class="tg-0pky">1972</td>
          </tr>
          <tr>
            <td class="tg-0pky"><span style="font-weight:bold">2D CNN</span></td>
            <td class="tg-0pky">96.95</td>
            <td class="tg-0pky">0</td>
            <td class="tg-0pky">11353</td>
            <td class="tg-0pky">37</td>
            <td class="tg-0pky">1993</td>
          </tr>
          <tr>
            <td class="tg-0pky"><span style="font-weight:bold">LSTM</span></td>
            <td class="tg-0pky">99.85</td>
            <td class="tg-0pky">530</td>
            <td class="tg-0pky">19</td>
            <td class="tg-0pky">89</td>
            <td class="tg-0pky">1745</td>
          </tr>
          <tr>
            <td class="tg-0pky"><span style="font-weight:bold">WLSTM</span></td>
            <td class="tg-0pky">98.79</td>
            <td class="tg-0pky">1054</td>
            <td class="tg-0pky">75</td>
            <td class="tg-0pky">26</td>
            <td class="tg-0pky">6379</td>
          </tr>
        </tbody>
      </table>
      <br>
      <br>
      <p>The LSTM model outperforms others in accuracy, thanks to its ability to handle sequential network traffic,
        learn long-term dependencies, and deal with noisy data. However, the windowed LSTM (WLSTM) didn't perform as
        well due to reduced temporal resolution. The 2D CNN had the worst accuracy but the fewest false positives (FPs),
        indicating it may capture spatial patterns in benign traffic, while MLP had a higher accuracy but more FPs,
        likely due to dataset imbalance. LSTM handles class imbalance better.
        <br>
        <br>
        Regarding inference time and runtime memory usage, MLP is the fastest (0.0023 ms) and LSTM is the slowest
        (0.0089 ms). This aligns with MLP's lightweight architecture. CNNs perform similarly, with LSTM using the least
        runtime memory. WLSTM is less memory-efficient due to windowed processing, but this trade-off can lead to faster
        inference times. All models have acceptable inference times below 25.74 milliseconds.
      </p>
      <div class="image-container">

        <img src="images/Accuracy.png" alt="Image 1" style="width: 300px; height: auto;">
        <img src="images/FalsePositives.png" alt="Image 2" style="width: 300px; height: auto;">
        <img src="images/MemoryUsage.png" alt="Image 1" style="width: 300px; height: auto;">
        <img src="images/InferenceTime.png" alt="Image 2" style="width: 300px; height: auto;">
      </div>
      In the analysis of model performance with varying numbers of input features (5, 15, 20, 30), it's observed that
      using all features results in high accuracies, but there's no consistent trend across all models when using fewer
      features. Surprisingly, the LSTM consistently maintains high accuracy with different feature counts, contrary to
      previous expectations.
      <br>
      <br>
      Concerning false positives (FPs), there's a "sweet spot" for all models, with the lowest FPs occurring when using
      15-20 features. This suggests that too few features may hinder learning, while using all features could introduce
      redundancy and confusion.
      <br>
      <br>
      MLP stands out by performing well with just five features, indicating its effectiveness with fewer inputs.
      However, models that yield low FPs but struggle to classify ransomware, like the 2D CNN, are less useful. In
      contrast, the LSTM maintains high accuracy even with 20 features.
      <br>
      <br>
      When considering runtime memory usage, as more features are used, it consistently increases across all models,
      aligning with prior research. Inference time changes slightly with different feature counts, with the LSTM being
      the slowest and the MLP the fastest. However, the number of features does not strongly influence inference time.
      <br>
      <br>
      In summary, varying input feature sizes impact accuracy, FPs, and runtime memory usage. The LSTM performs well
      across different input sizes, suggesting the sequential nature of network packet data. The optimal range for
      minimizing FPs is using 50% to 66% of the feature set. Larger inputs lead to increased runtime memory usage.
      <br>
      <p class="Abstract"> Conclusion </p>
      <p>In conclusion, this study evaluated the performance of various ML models for classifying encrypted network
        traffic as ransomware.
        <br>
        <br>
        We found that there is no universally perfect model, and the choice should align with specific network
        requirements. Additionally, we observed that the number of features inputted into a model affects model
        performance, with the 20-feature LSTM model consistently performing well. Dimensionality reduction did not
        greatly impact inference time but increased runtime memory usage. Lastly, combining a sliding window of inputs
        with an LSTM did not yield favourable results. Our findings emphasize the need for tailored model selection,
        optimized input sizes, and carefully designed feature sets for effective ransomware detection in encrypted
        network traffic.
        <br>
        <br>
        For future work, network traffic could be collected from multiple networks to make the models more generalisable
        to different network environments. Secondly, to further investigate dimensionality reduction, rather than
        exclusively selecting the first x features, future studies could involve randomly selecting features to assess
        the significance of feature quantity versus feature selection. Extending dimensionality reduction experiments to
        different network traffic datasets can also test whether similar effects manifest across various datasets. To
        reduce the inaccuracies when measuring runtime memory usage and inference time, future work could further
        include measuring these metrics in a more isolated environment, where these metrics can easily be controlled.
      </p>
    </div>
    <!-- Content for Tab 4-->
    <div id="tab4-content" class="tab-content">
      <!-- First Row with Two PDFs -->
      <div class="pdf-row">
        <div class="pdf-container">
          <h3>PDF 1</h3>
          <embed src="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf"
            type="application/pdf">
          <a href="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" target="_blank"
            class="view-pdf-button">View PDF</a>
        </div>
        <div class="pdf-container">
          <h3>PDF 2</h3>
          <embed src="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" type="application/pdf">
          <a href="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" target="_blank" class="view-pdf-button">View PDF</a>
        </div>
      </div>

      <!-- Second Row with Two PDFs -->
      <div class="pdf-row">
        <div class="pdf-container">
          <h3>PDF 3</h3>
          <embed src="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" type="application/pdf">
          <a href="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" target="_blank" class="view-pdf-button">View PDF</a>
        </div>
        <div class="pdf-container">
          <h3>PDF 4</h3>
          <embed src="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" type="application/pdf">
          <a href="documents/LMPCHR002-DeepLearningForRansomwareDetection_ChrisLamprecht_FINAL.pdf" target="_blank" class="view-pdf-button">View PDF</a>
        </div>
      </div>
    </div>


    <div id="tab5-content" class="tab-content"> Contact Details </div>
</body>

</html>